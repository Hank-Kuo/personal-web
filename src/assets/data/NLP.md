# 自然語言處理 — 使用 N-gram 實現輸入文字預測
## Introduce
在學習自然語言處理 (Natural Language Processing) 時，經常會從語言模型 (Language Model) 開始學起，N-gram 便是一個入門常見語言模型。在這篇文章將帶你一探究竟 N-gram，並且利用 N-gram 實作一個模型，能夠像似搜尋引擎根據使用者的輸入推薦相關的字詞。

## 語言模型 (Language Model) 與 N-gram 原理
語言模型這 4 個字看似很博大精深，但是它僅僅指的就是「一個句子的機率」，在一個句子中的每個字，我們可以給它 P(S)，代表每個字的機率。

假設我們給定一個句子「花博即將在台中__ 」，在留空的地方一般人都會想到該填入「舉行」，也就是當出現「花博即將在台中」，電腦能夠推斷出在這句話的後方出現「舉行」的機率最高。

所以我們假設第 i 個字會跟第 1 個字到 i-1 個字有關，這時你會發現如果這個句子很長，該機率計算量將會變得非常大，因此我們可以利用 Markov 假設，也就是當前的這個字僅僅與前幾個有限的字相關，如此一來，將不必追溯至第 1 個字，可以減少該機率的計算量：

當 m = 3 時，也就是第 i 個字與前 3 個字有相關，稱作 3-gram 或 trigram。用上述的例子舉例，我們要找一個字會令 P(w|在台中) 的機率最高，假設推算出 P(舉|在台中) 的機率最高後，再繼續找出另一個字讓 P(w|台中舉) 的機率最高。

這時你會想該如何計算 P(w|在台中) 這種條件機率？我們可以使用最大似然估計 (Maximum Likelihood Estimation )，詳細推導可以看這篇 🔗文章：

藉由計算字詞出現的次數來推估 N-gram 的機率，所以我們要做的事情是計算「在台中 w」與「在台中」分別出現的次數，再相除得到 P(w|在台中) 的機率。

## 從零開始實現 N-gram
在這個範例中，我們會使用 Open data 作為資料集，並且利用 N-gram 訓練一個語言模型。
我們會用到 Collections 中的 namedtuple 與 Counter。namedtuple 提供的功能是像 C 語言的 struct，能夠清楚表達一個資料的結構；而 Counter 則是能夠快速地幫助我們計算數量，例如 list 中每個元素的數量各有幾個。

## 引入資料集
在這篇文章中使用的資料集是政府資料開放平台提供的 🔗桃園市官網市政新聞，由於 N-gram 所產生的結果會根據資料集而有所不同，因此在選擇資料集時必須評估該資料集是否合適。假設使用的資料集中包含許多火星文，根據 N-gram 產生的語言模型，其結果將會與火星文相關。

## 結論
N-gram 是一個理論較為簡單，而且容易實作的語言模型，可以花少量的程式碼達到不錯的效果。
但是，也許你會思考在預測下一個字時，得出字組合起來沒有特別的意思，因為 N-gram 只是單純利用統計得出機率比較大的字詞組合，所以在預測時看似會較沒有邏輯。
如果想要得出更好的結果可以嘗試使用較為複雜的模型，例如 Transformer 或是 ConvS2S 等 Google 與 Facebook 曾經發表的模型。

## Reference
- 自然語言處理中的n-gram中模型詳解
- 深入淺出講解語言模型
- 漫談Language Model (1): 原理篇
- Language model
- 4 1 Introduction to N grams
- Maximum Likelihood Estimation of N-Gram Model Parameters
